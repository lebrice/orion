# -*- coding: utf-8 -*-
"""Orion + Brax Training with PyTorch on GPU

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KlaTeScstmRg7AIWLgrXy9zGmayb5zMS

# Training in Brax with PyTorch on GPUs

Brax is ready to integrate into other research toolkits by way of the [OpenAI Gym](https://gym.openai.com/) interface.  Brax environments convert to Gym environments using either [GymWrapper](https://github.com/google/brax/blob/main/brax/envs/wrappers.py) for single environments, or [VectorGymWrapper](https://github.com/google/brax/blob/main/brax/envs/wrappers.py) for batched (parallelized) environments.
"""
from __future__ import annotations

import os
os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "False"
# @title Import Brax and some helper modules
# from IPython.display import clear_output
# !pip install --upgrade matplotlib
from datetime import datetime
from typing import (
    Dict,
    List,
    TypeVar,
)

import matplotlib.pyplot as plt
import numpy as np
import tqdm
from brax import envs
from brax.training import ppo

from dataclasses import dataclass
from dataclasses import dataclass

from simple_parsing.helpers.hparams import HyperParameters
from simple_parsing.helpers.hparams.hparam import log_uniform, uniform
from ppo_pytorch import train
from orion.client import build_experiment
from orion.core.worker.trial import Trial
from pathlib import Path
src_dir = Path(__file__).parent


"""Here is a PPO Agent written in PyTorch:"""


T = TypeVar("T")



@dataclass
class HParams(HyperParameters):
    discounting: float = uniform(0.5, 0.999, default=0.97)
    learning_rate: float = log_uniform(1e-6, 1e-2, default=3e-4)
    entropy_cost: float = log_uniform(1e-4, 1e-1, default=1e-2)
    max_steps: int = 1_000_000


def run(hparams: HParams, use_pytorch: bool = True) -> float:
    print("Running with hparams: ", hparams.dumps_json(indent="\t"))
    train_sps = []
    ydata = []
    xdata = []
    ydata = []
    eval_sps = []
    train_sps = []
    times = [datetime.now()]
    total_steps = 10_000_000

    pbar = tqdm.tqdm(desc="Training", total=total_steps)
    plt.ion()

    last_step = 0

    def progress(num_steps: int, metrics: Dict):
        times.append(datetime.now())
        xdata.append(num_steps)
        ydata.append(metrics["eval/episode_reward"])
        eval_sps.append(metrics["speed/eval_sps"])
        train_sps.append(metrics["speed/sps"])
        nonlocal last_step
        pbar.update(num_steps - last_step)
        last_step = num_steps
        pbar.set_postfix(
            {
                "steps per second": train_sps[-1],
                "reward": ydata[-1],
            }
        )
        plt.xlim([0, total_steps])
        plt.ylim([0, 6000])
        plt.xlabel("# environment steps")
        plt.ylabel("reward per episode")
        plt.plot(xdata, ydata)
        plt.show()
        # plt.draw()

    config = dict(
        reward_scaling=0.1,
        episode_length=1000,
        unroll_length=5,
        num_minibatches=32,
        num_update_epochs=4,
        num_envs=2048,
        batch_size=1024,
    )
    if use_pytorch:
        _ = train(
            env_name="ant",
            device="cpu",
            num_timesteps=total_steps,
            discounting=hparams.discounting,
            learning_rate=hparams.learning_rate,
            entropy_cost=hparams.entropy_cost,
            progress_fn=progress,
            **config,
        )
    else:
        _ = ppo.train(
            environment_fn=envs.create_fn(env_name="ant"),
            num_timesteps=total_steps,
            discounting=hparams.discounting,
            learning_rate=hparams.learning_rate,
            entropy_cost=hparams.entropy_cost,
            normalize_observations=True,  # TODO: add this to the pytorch version.
            log_frequency=10,  # TODO: add this to the pytorch version.
            action_repeat=1,  # TODO: add this to the pytorch version.
            progress_fn=progress,
            **config,
        )
    pbar.close()

    print(f"train steps/sec: {np.mean(train_sps[1:])}")
    average_eval_reward = np.mean(ydata[-100:])
    print(f"Average of last 100 eval rewards: {average_eval_reward}")
    return average_eval_reward


def main():
    search_space = HParams.get_orion_space_dict()
    search_space["max_steps"] = "fidelity(10_000, 10_000_000, base=10)"
    experiment = build_experiment(
        name="brax",
        space=search_space,
        max_trials=1000,
        storage=dict(type="pickleddb", host=str(src_dir / "db.pkl")),
        algorithms={"evolutiones": {}},
        debug=True,
    )
    trials: List[Trial] = experiment.fetch_trials()
    objectives = [trial.objective for trial in trials]
    print(f"{len(trials)} previous trials.")
    try:
        while not experiment.is_done:
            # Get the next trial to try:
            trial = experiment.suggest()
            hparams = HParams(**trial.params)
            # hparams = HParams.sample()
            average_eval_reward = run(hparams=hparams, use_pytorch=False)

            results = [
                dict(
                    name="-average_eval_reward",
                    type="objective",
                    value=-average_eval_reward,
                )
            ]
            print(results)
            experiment.observe(trial, results)

    except KeyboardInterrupt:
        print(f"Exit.")
        exit()

if __name__ == "__main__":
    main()
